{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Logistic\n",
    "\n",
    "In this part, you will complete a Do-It-Yourself (DIY) implementation of binary logistic regression in an object-oriented pattern that corresponds with the Scikit-Learn API.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Write object-oriented code for a Python class, matching standard API patterns.\n",
    "2. Apply numerical Python (NumPy) to efficiently implement binary logistic regression, including code to fit the model to data using the gradient descent algorithm. \n",
    "3. Evaluate your implementation compared to the Scikit-Learn standard on synthetic data. \n",
    "4. Perform an ablation study on the impact of the learning rate hyperparameter for fitting a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "First, we will use Scikit-Learn to develop a baseline logistic regression model to which we can compare our DIY implementation. Run the following code to generate synthetic data for use in this part of the assignment. Observe that the predictive target is coded as 0 or 1, that the `sigmoid` function is defined for you, and that the code also splits the synthetic data into train and test sets for you.\n",
    "\n",
    "Use Scikit-Learn to fit a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#logisticregression) on the train set with the parameter setting `penalty = 'None'` (this will train a basic model without applying any regularization). Evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of the model on both the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "np.random.seed(2024)\n",
    "n = 1000\n",
    "features = 20\n",
    "\n",
    "X = np.random.normal(size = (n, features))\n",
    "weights = np.random.normal(size = features)\n",
    "probs = sigmoid(X @ weights + np.random.normal(scale=0.01)) \n",
    "y = np.random.binomial(n=1, p=probs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8714285714285714\n",
      "Test accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(penalty=None, random_state=2024)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on both the train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on both sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Complete the following class to implement binary logistic regression. Some important notes about the implementation:\n",
    "\n",
    "1. Remember that the Scikit-Learn API treats an input `X` array, whether to `fit` or `predict`, as a matrix with a row for every data point and a column for every feature. \n",
    "2. For `fit`, every row in `X` corresponds to a given output in `y`, and  you don't need to return anything, just optimize the internal model weights (which should be stored as instance variables). For `predict_proba` and `predict`, you should return a NumPy array with one element (corresponding to a probability or a 0/1 value) for every row in the input `X`.\n",
    "3. Remember that logistic regression models the probability of outputting `1` as a sigmoid of a linear function of features. This has several implications. One is that the number of weights in your model should equal the number of features equal to the number of columns of the `X` matrix passed to the `fit` method. We recommend that you initialize these weights as random normally distributed values, for example by using NumPy's [random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html). Another implication is that `predict_proba` should return the sigmoid activation of the [dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) (multiply element-wise then add together) of your model's weights and the given input. You do **not** need to include a bias term for this implementation.\n",
    "4. For the `predict` function, you should use a simple thresholding of 0.5. That is, you should calculate the probabilities using the `predict_proba` method and return `1` if the probability is greater than 0.5 and `0` otherwise. You can assume that `y` will consist exclusively of `0`s or `1`s for the purpose of this implementation.\n",
    "5. The `fit` method should implement gradient descent on the log-likelihood. For a given feature/weight dimension $j$ and a particular data point $x$ with label $y$, the partial derivative with respect to weight $w_j$ is $(a - y)x_j$ where $a$ is the activation (the predicted probability) assosciated with example $x$. For each feature, this quantity should be averaged over all training data. The vector of all such values forms the gradient $\\vec{\\nabla}$. The gradient descent learning update should then be $\\vec{w'} = \\vec{w} - \\eta \\vec{\\nabla}$ where $\\eta$ is the learning rate `lr` passed to the constructor, $\\vec{w}$ are the previous weights and $\\vec{w'}$ are the weights for the next iteration. The algorithm should proceed for `iters` iterations.\n",
    "6. You will note the `fit` method takes an optional `verbose` parameter. While it is not required, we highly recommend that you include code in the `fit` method that, when `verbose` is `True`, provides additional logging or printing of information about the training process to help debug. \n",
    "7. The `pass` statements are syntactic placeholders that should be removed when you implement a method.\n",
    "8. Finally, note that your implementation will be much more efficient if you use vectorized NumPy operations and avoid for loops or nested for loops over large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, lr=0.1, iters=100, random_state=2024):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.random_state = random_state \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X @ self.weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.weights = np.random.normal(size=X.shape[1])\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            proba = self.predict_proba(X)\n",
    "            gradient = np.dot(X.T, proba - y) / len(y)\n",
    "            self.weights -= self.lr * gradient\n",
    "            \n",
    "            if verbose and i % 100 == 0:\n",
    "                loss = -np.mean(y * np.log(proba) + (1 - y) * np.log(1 - proba))\n",
    "                print(f\"Iteration {i}: Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Use your DIY `BinaryLogisticRegression` class from task 2 to fit a logistic regression model on the train set as you did for the Scikit-Learn implementation in task 1. Use the default parameters (`lr=0.1, iters=1000, random_state=2024`). Evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of your DIY model on both the train set and the test set. You should achieve similar performance compared to the Scikit-Learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (DIY logistic regression): 0.8671428571428571\n",
      "Test accuracy (DIY logistic regression): 0.8766666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "my_model = BinaryLogisticRegression(lr=0.1, iters=1000, random_state=2024)\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = my_model.predict(X_train)\n",
    "y_test_pred = my_model.predict(X_test)\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training accuracy (DIY logistic regression): {train_accuracy}\")\n",
    "print(f\"Test accuracy (DIY logistic regression): {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Perform an *ablation* study on the learning rate hyperparameter `lr`. Specifically, try fitting 20 different models using your DIY `BinaryLogisticRegression` implementation, trying every combination of parameter settings `[10, 1, 0.1, 0.01, 0.001]` for the learning rate `lr` and `[1, 5, 20, 100]` for the number of gradient descent iterations `iters`. For each combination, evaluate the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of the model on the train set only (note that hyperparameters should never be selected using the test data).\n",
    "\n",
    "Report all of your accuracies in a clearly labeled [Markdown table](https://www.markdownguide.org/extended-syntax/#tables). For readability, only report three digits per entry; for example, if the accuracy of a run is `0.8747340`, write `0.874` or `87.4` in the table. Explain which setting of `lr` you would use in this case and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Learning Rate (lr)  Iterations (iters)  Train Accuracy\n",
      "0               10.000                   1           0.841\n",
      "1               10.000                   5           0.870\n",
      "2               10.000                  20           0.869\n",
      "3               10.000                 100           0.867\n",
      "4                1.000                   1           0.629\n",
      "5                1.000                   5           0.716\n",
      "6                1.000                  20           0.850\n",
      "7                1.000                 100           0.866\n",
      "8                0.100                   1           0.617\n",
      "9                0.100                   5           0.620\n",
      "10               0.100                  20           0.654\n",
      "11               0.100                 100           0.786\n",
      "12               0.010                   1           0.614\n",
      "13               0.010                   5           0.616\n",
      "14               0.010                  20           0.617\n",
      "15               0.010                 100           0.627\n",
      "16               0.001                   1           0.614\n",
      "17               0.001                   5           0.614\n",
      "18               0.001                  20           0.614\n",
      "19               0.001                 100           0.617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lrs = [10, 1, 0.1, 0.01, 0.001] \n",
    "iters_list = [1, 5, 20, 100]\n",
    "results = []\n",
    "\n",
    "for lr in lrs:\n",
    "    for iters in iters_list:\n",
    "        my_model = BinaryLogisticRegression(lr=lr, iters=iters, random_state=2024)\n",
    "        my_model.fit(X_train, y_train)\n",
    "        y_train_pred = my_model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        results.append([lr, iters, round(train_accuracy, 3)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Learning Rate (lr)\", \"Iterations (iters)\", \"Train Accuracy\"])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table suggests that a learning rate of 10 maximizes training accuracy, regardless of the number of iterations. The training accuracy of every other value for lr pales in comparison to that of lr=10, so that is the value I'll select here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
